{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install evaluate\n",
    "# pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99988eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce67cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('doushabao4766/msra_ner_k_V3')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "for items in ds['train']:\n",
    "    print(f\"tokens: {items['tokens']}, ner_tags: {items['ner_tags']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c527b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证tag标签数量\n",
    "tags_id = set()\n",
    "for item in ds['train']:\n",
    "    tags_id.update(item['ner_tags'])\n",
    "tags_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag对应字符串处理\n",
    "entities = ['0'] + list({'PER', 'LOC', 'ORG'})\n",
    "tags = ['0']\n",
    "for entity in entities[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "entity_idx = {entity:i for i, entity in enumerate(entities)}\n",
    "print(tags)\n",
    "print(entity_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_input_proc(item):\n",
    "    # 中文文本分词于英文不同(word_ids)\n",
    "    # 利用is_split_into_words=True,导入已拆分的中文字符\n",
    "    input_data = tokenizer(item['tokens'],\n",
    "                           truncation = True,\n",
    "                           add_special_tokens = False,\n",
    "                           max_length = 512,\n",
    "                           is_split_into_words = True\n",
    "                          )\n",
    "    # 标签长度也要截取\n",
    "    labels = [lbl[:512] for lbl in item['ner_tags']]\n",
    "    input_data['labels'] = labels\n",
    "    return input_data\n",
    "    \n",
    "ds1 = ds.map(data_input_proc, batched = True)\n",
    "print(ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.set_format('torch', columns = ['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "for item in ds1['train']:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建模\n",
    "id2lbl = {i: tags for i, tags in enumerate(tags)}\n",
    "lbl2id = {tags : i for i, tags in enumerate(tags)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-chinese',\n",
    "                                                        num_labels = len(tags),\n",
    "                                                        id2label = id2lbl,\n",
    "                                                        label2id = lbl2id\n",
    "                                                       )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348dee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数\n",
    "args = TrainingArguments(\n",
    "    output_dir = 'msr_ner_train',\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    report_to = 'tensorboard',\n",
    "    eval_strategy = 'epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(result):\n",
    "    # 获取评估对象\n",
    "    seqeval = evaluate.load('seqeval')\n",
    "    predicts, labels = result\n",
    "    predicts = np.argmax(predicts, axis = 2)\n",
    "\n",
    "    # 准备评估数据\n",
    "    predicts = [\n",
    "        [tags[p] for p,l in zip(ps, ls) if l != -100]\n",
    "        for ps, ls in zip(predicts, labels)\n",
    "    ]\n",
    "    labels = [\n",
    "        [tags[l] for p,l in zip(ps, ls) if l != -100]\n",
    "        for ps, ls in zip(predicts, labels)\n",
    "    ]\n",
    "    results = seqeval.compute(predictions = predicts, references = labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc50c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = ds1['train'],\n",
    "    eval_dataset = ds1['test'],\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928752f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline('token-classification', 'msr_ner_train/checkpoint-2112')\n",
    "words_result = pipeline('双方确定了今后发展中美关系的指导方针')\n",
    "\n",
    "entity_result = []\n",
    "for result in words_result:\n",
    "    if result['entity'] != '0':\n",
    "        entity_result.append(result)\n",
    "print(entity_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
